function [gparams,memberships] =  gaussian_mixture(data,K,init_method,epsilon, niterations,plotflag, RSEED)
% [gparams,memberships] =  gaussian_mixture(data,K,init_method,epsilon, 
%                                                niterations,plotflag, RSEED)
%
% Template for a function to fit  a Gaussian mixture model via EM 
% using K mixture components. The data are contained in the N x d "data" matrix. 
%  
% INPUTS
%  data: N x d real-valued data matrix
%  K: number of clusters (mixture components)
%  initialization_method: 1 for memberships, 2 for parameters, 3 for kmeans 
%  epsilon: convergence threshold used to detect convergence
%  niterations (optional): maximum number of iterations to perform (default 500)
%  plotflag (optional): equals 1 to plot parameters during learning, 
%                       0 for no plotting (default is 0)
%  RSEED (optional): initial seed value for the random number generator
%  
%
% OUTPUTS
%  gparams:  2d structure array containing the learned mixture model parameters: 
%           gparams(k).weight = weight of component k
%           gparams(k).mean = d-dimensional mean vector for kth component 
%           gparams(k).covariance = d x d covariance vector for kth component
%
%  memberships: n x K matrix of probability memberships for "data"
%
%  Note: Interpretation of gparams and memberships:
%    - gparams(k).weight is the probability that a randomly selected row
%         belongs to component (or cluster) i (so it is "cluster size")
%    - memberships(i,k) = p(cluster k | x) which is the probability
%         (computed via Bayes rule) that vector x was generated by cluster
%         k, according to the "generative" probabilistic model. 

% your code goes here....

% initialize....

% perform E-step...

% perform M-step...

% compute log-likelihood and print to screen.....

% check for convergence.....

%does k-means
datasetSize = size(dataset);
numPoints = datasetSize(1);

%randomizes the order of the operant data set
dataset = dataset(randperm(numPoints),:);

%repeats k-means for r different starting points
randOrder = randperm(numPoints);
currentMinSumOfSquares = inf;
for instance = 1:r
   
    meanRows = randOrder((k*instance - k + 1):(k*instance));

    %assign the cluster rows randomly
    currentClusters = zeros(k,2);
    currentClusterDistances = zeros(k,1);
    for row = 1:k
        currentClusters(row,:) = dataset(meanRows(row),:);
    end

    currentClusterAssignments = zeros(numPoints,1);

    for iteration = 1:maxiterations
        currentNumChanged = 0;
        currentSumOfSquares = 0;
        cluster1Rows = [];
        cluster2Rows = [];

        %loop through the data points finding the closest cluster for each point
        for dataPoint = 1:numPoints
           for cluster = 1:k
               currentClusterDistances(cluster) = norm(currentClusters(cluster,:)-dataset(dataPoint,:));
           end

           [currentMinDistance,bestCluster] = min(currentClusterDistances);
           currentSumOfSquares = currentSumOfSquares + currentMinDistance;

           if(bestCluster ~= currentClusterAssignments(dataPoint))
               currentClusterAssignments(dataPoint) = bestCluster;
               currentNumChanged = currentNumChanged + 1;
           end

           if(currentClusterAssignments(dataPoint) == 1)
              cluster1Rows = [cluster1Rows;dataset(dataPoint,:)]; 
           elseif(currentClusterAssignments(dataPoint) == 2)
               cluster2Rows = [cluster2Rows;dataset(dataPoint,:)]; 
           end
        end

        %it has converged
        if(currentNumChanged < 1)
           break; 
        end

        %reassign the clusters
        currentClusters(1,:) = [mean(cluster1Rows(:,1)) mean(cluster1Rows(:,2))];
        currentClusters(2,:) = [mean(cluster2Rows(:,1)) mean(cluster2Rows(:,2))];
    end

    
    if(currentSumOfSquares < currentMinSumOfSquares)
        finalCluster1Rows = cluster1Rows;
        finalCluster2Rows = cluster2Rows;
    end 
    
end

end

