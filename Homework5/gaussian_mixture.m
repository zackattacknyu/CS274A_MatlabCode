function [gparams,memberships] =  gaussian_mixture(data,K,init_method,epsilon, niterations,plotflag, RSEED)
% [gparams,memberships] =  gaussian_mixture(data,K,init_method,epsilon, 
%                                                niterations,plotflag, RSEED)
%
% Template for a function to fit  a Gaussian mixture model via EM 
% using K mixture components. The data are contained in the N x d "data" matrix. 
%  
% INPUTS
%  data: N x d real-valued data matrix
%  K: number of clusters (mixture components)
%  initialization_method: 1 for memberships, 2 for parameters, 3 for kmeans 
%  epsilon: convergence threshold used to detect convergence
%  niterations (optional): maximum number of iterations to perform (default 500)
%  plotflag (optional): equals 1 to plot parameters during learning, 
%                       0 for no plotting (default is 0)
%  RSEED (optional): initial seed value for the random number generator
%  
%
% OUTPUTS
%  gparams:  2d structure array containing the learned mixture model parameters: 
%           gparams(k).weight = weight of component k
%           gparams(k).mean = d-dimensional mean vector for kth component 
%           gparams(k).covariance = d x d covariance vector for kth component
%
%  memberships: n x K matrix of probability memberships for "data"
%
%  Note: Interpretation of gparams and memberships:
%    - gparams(k).weight is the probability that a randomly selected row
%         belongs to component (or cluster) i (so it is "cluster size")
%    - memberships(i,k) = p(cluster k | x) which is the probability
%         (computed via Bayes rule) that vector x was generated by cluster
%         k, according to the "generative" probabilistic model. 

dataset = data;
method = init_method;


datasetSize = size(dataset);
numPoints = datasetSize(1);
numDimensions = datasetSize(2);
dataset = dataset(randperm(numPoints),:);



if(method == 1)
    
    %method 1, random membership probabilities
    
    memberProbs = initValuesMethod1(dataset,K);
    %init M step
    alphaValues = computeNewAlphaValues(numPoints,memberProbs);
    muVector = computeNewMuValues(dataset,memberProbs,K);
    sigmaVector = computeNewSigmaValues(dataset,memberProbs,K,muVector);
    
elseif(method == 2)
   
    %method 2, random points for the means
    randOrder = randperm(numPoints);
    muVector = dataset(randOrder(1:K),:);
    overallCov = cov(dataset);
    sigmaVector = zeros([numDimensions numDimensions K]);
    for k = 1:K
       sigmaVector(:,:,k) = overallCov; 
    end
    alphaValues = rand([1 K]);
    alphaValues = alphaValues./sum(alphaValues);
    
elseif(method == 3)
    
    maxiterations = 10;
    r = 10;
    [~,~,finalClusters] = ...
    kMeansCluster(dataset,K,r,maxiterations);

    muVector = finalClusters;
    overallCov = cov(dataset);
    sigmaVector = zeros([numDimensions numDimensions K]);
    for k = 1:K
       sigmaVector(:,:,k) = overallCov; 
    end
    alphaValues = rand([1 K]);
    alphaValues = alphaValues./sum(alphaValues);
    
end

firstLikelihood = computeLogLikelihood( dataset, alphaValues, K...
        , muVector, sigmaVector );
previousLikelihood = 0;
maxiterations = 400;
likelihoods = zeros(1,maxiterations);
for iteration = 1:maxiterations
    
    %does the E-step
    memberProbs = computeMemberProbs(dataset,alphaValues,K,...
        muVector,sigmaVector);
    
    %does the M-step
    alphaValues = computeNewAlphaValues(numPoints,memberProbs);
    muVector = computeNewMuValues(dataset,memberProbs,K);
    sigmaVector = computeNewSigmaValues(dataset,memberProbs,K,muVector);

    currentLikelihood = computeLogLikelihood( dataset, alphaValues, K...
        , muVector, sigmaVector );
    
    likelihoods(iteration) = currentLikelihood;
    
    %possible converge criteria
    if( abs(currentLikelihood-previousLikelihood) < epsilon*(abs(currentLikelihood-firstLikelihood)) )
       break; 
    end
    
    previousLikelihood = currentLikelihood;
end

likelihoods = likelihoods(1:iteration);

plot(likelihoods);

memberships = memberProbs;

end

