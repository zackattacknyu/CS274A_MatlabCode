\documentclass[11pt,psfig]{article}
\usepackage{epsfig}
\usepackage{times}
\usepackage{amssymb}
\usepackage{float}

\newcount\refno\refno=1
\def\ref{\the\refno \global\advance\refno by 1}
\def\ux{\underline{x}}
\def\ut{\underline{\theta}}
\def\umu{\underline{\mu}}
\def\be{p_e^*}
\newcount\eqnumber\eqnumber=1
\def\eq{\the \eqnumber \global\advance\eqnumber by 1}
\def\eqs{\eq}
\def\eqn{\eqno(\eq)}

 \pagestyle{empty}
\def\baselinestretch{1.1}
\topmargin1in \headsep0.3in
\topmargin0in \oddsidemargin0in \textwidth6.5in \textheight8.5in
\begin{document}
\setlength{\parskip}{1.2ex plus0.3ex minus 0.3ex}


\thispagestyle{empty} \pagestyle{myheadings} \markright{Homework
5: CS 274A, Probabilistic Learning: Winter 2014}



\title{CS 274A Homework 5}
\author{Probabilistic Learning: Theory and Algorithms, CS 274A, Winter 2014}
\date{Due Date: Wednesday March 5th: report in class, code uploaded to EEE}

\maketitle




%\input{../guidelines.tex}
 
\begin{itemize}
\item For this homework all code should be submitted to the Homework 5 dropbox in EEE, as a single Zip file with your name. Each MATLAB function should be in its own separate file.  If you are not using MATLAB, you should also provide us with a README file, and possibly a script, that tells us how we can execute/call your code.
    
\item The written report (with graphs, etc) should be submitted in class.

\item Required reading is Note Set 4 on the class Web page.
Optional additional reading is  
and Section 20.3 on Gaussian mixtures in the Barber text and
 pages 351--357 in the Murphy text.
 
\item This  homework consists of quite a bit of programming. If you take it
step by step it is straightforward, but be sure to start working on this well
in advance of the deadline. 
     
\end{itemize}

 \newpage





\section*{MATLAB: K-Means and Gaussian Mixture Learning}



\subsection*{Data Sets} There are 3 data sets for
experimentation: all are two-dimensional so that you can
visualize the results. The data format is 1 data point per row, each column
corresponds to a feature. The data sets are available from the class Web page.
Data set 3 also has class labels, called {\tt labelset3},
 which are not to be used in clustering but just for evaluating
your results afterwards.  You should
 read the data into MATLAB and plot some scatter
plots, before you do any clustering, just so that you have an idea of where the
data are in various 2-dimensional plots of the data. 
\begin{enumerate}
\item Data set 1: this is 2d data simulated from 2 Gaussians, with
some degree of overlap. The first 800 points belong to 1 Gaussian,
the 2nd to the other Gaussian.

\item Data set 2: this is 2d data simulated from 3 Gaussians, with
considerable overlap. There are 500 points from each Gaussian,
ordered together in the file.

\item Data set 3: this a real 2d data set that I have worked on in the past in collaboration with Professor
Christine McLaren, Department of Epidemiology, UCI. Each data point represents
an individual person: one group is normal healthy people, the other has iron
deficient anemia (so the true value of $K$ is 2). The two measurements on each
individual are their mean red blood-cell volume and their mean hemoglobin
concentration.  Both measurements tend to be lower for anemic individuals. The
class labels are provided in the file labelset3. 


For the data sets above you are to run the clustering
algorithms in an unsupervised manner, i.e., without class labels or knowledge of what the true classes are. 
You can then
use the labels after clustering if you wish to check to see how the clustering
matches with ``truth."

\item Image Data: (Optional, this is just for fun if you wish to play with this data---no need to
submit anything related to this data set). This data consists of the pixels in a small JPG image {\tt clown.jpg}. You will convert this to an array where each row corresponds to a pixel and there are 3 columns, one for the r, g, and b channels of color information. The image is small to keep the number of pixels to a manageable size---but there are still over 30,000 pixels in total. There are no ``true classes" here for the pixels. K-means and/or Gaussian mixtures can be used to compress the real-values in (r,g,b) into a ``dictionary" of $K$ clusters (effectively using clustering to perform data compression). You could experiment with different values of $K$ and look at the visual difference between the compressed image (where you replace each pixel by the mean of the cluster it was assigned to) and the original image, as a function of $K$.  The MATLAB directory for this homework contains a simple function to read in and  to display the (r,g,b) pixel values. 

\end{enumerate}
     
You should randomize the order of the rows (e.g., use {\tt randperm}) for all of these data sets before you run your clustering algorithms,  to avoid any possible biasing of your algorithm (e.g, during initialization) due to systematic effects based on row order.




%You may also want to use the same general array structure for Gaussian
%parameters and data matrix formats that we used in earlier homeworks, unless you have
%a preference to use your own data structures. Keep in mind that for earlier homeworks
%we knew the class labels: here we are estimating the class memberships (or
%cluster memberships) in an unsupervised manner from the  data.


\subsection*{Algorithm 1: $K$-means Clustering}
Implement the standard version of the algorithm as described in Note Set 4.
The initial starting points for the $K$ cluster means can be $K$ randomly selected data points. You should
 have an option to run the algorithm $r$ times from $r$ different
randomly chosen initializations (e.g., $r = 10$), where you then select the solution  that gives the lowest sum of squares error over the  $r$ runs.  

\subsection*{Algorithm 2: Gaussian Mixture Clustering}


Gaussian mixture clustering is a probabilistic approach to clustering where 
 we assume a Gaussian mixture model of $K$ components
for the data and we find the parameters of the model using maximum likelihood, as described in Note Set 4.  


The E-step provides the ``membership" probabilities  for each data point for
each class: so we should have an $n \times K$ matrix of probabilities, where
each row sums to 1. On the right-hand side of the E-step equations  we use the
most recently available parameter estimates (those from the most recent
M-step).


The ``M-step" equations provide new parameter estimates at each iteration (the
left hand sides of the equations). It is important to compute the equations in
the order given. The M-step equations are computed $K$ times at each iteration,
once for each mixture component/cluster.


I recommend that you use the following general outline
for your Gaussian mixture MATLAB code:
\begin{itemize}
\item Initialize the parameters (e.g., using the first method described under Initialization Methods below)
\item Execute E and M steps as long as the convergence condition is not satisfied:
\begin{itemize}
\item E-step: compute membership probabilities using the current
$\theta^{current}$ values.  
\item M-step: compute new parameters $\theta^{new}$
using the membership probabilities from the E-step 
%\item Convergence condition:
%Calculate the log-likelihood using $\theta^{new}$ and check if the relative
%increase since the last iteration is below some threshold (see below for more
%details). If so, halt and return the current parameters. If not, continue to
%iterate
\end{itemize}
\item After each EM iteration compute the log-likelihood of the data using $\theta^{new}$  (so that you can print   out the log-likelihood values from each iteration as the algorithm is running and monitor its convergence) and compute your convergence criterion (see discussion below of different possible convergence criteria you could use). If the convergence criterion is not satisfied, then execute another EM iteration. 
\end{itemize}


I also recommend that your algorithm runs this process multiple times from $r$ different
randomly-chosen starting conditions (e.g. $r=10$) and pick the solution
that results in the highest log-likelihood (since EM
in general only finds local maxima).  

\subsubsection*{\bf Options for Initializing EM}
\begin{enumerate}
\item For each of the $n$ data vectors, generate $K$ random membership probabilities (making sure they are normalized to sum to 1) and then use an initial M-step to generate an initial set of parameters. Note that for large data sets in particular this will tend to start the algorithm in roughly the same place in parameter space for different random runs (you may want to think about why this is so)---so you may want to try also initializing with the methods below to get higher diversity in terms of where the algorithm is starting from in parameter space.
\item Randomly select initial parameter values and then start with an E-step. You can select the $K$ initial means simply by picking $K$ data points at random. For the initial covariances you can compute the overall covariance of the data, and just use that, or use a scaled version of it (e.g., reduce all the variances/covariances by some factor)---using the overall covariance of the data should be fine. The initial weights for the components could be uniform or random.
\item You could first run K-means and use the results to initialize either the memberships or the parameters for EM. This is commonly done in practice. It often helps EM for Gaussian mixtures in the sense that the means in the K-means solution can often be close to the means in the final EM mixture solution. But there are also cases where initializing with K-means can lead to worse results than initializing randomly (all of these initialization techniques are just heuristics and not guaranteed to always work).
\end{enumerate}




%
%
%For example, for each start, select the
%initial $K$ Gaussian means by randomly selecting $K$ initial data points, and
%select the initial $K$ covariances as all being some multiple of the overall
%data covariance---the selection of initial covariances is not as critical as
%the initial means). Another option for initialization is to randomly assign
%class labels to the training data points and then calculate $\theta^0$ based on
%this initial random assignment (or begin the iterations by executing a single
%M-step, which is also fine).



The EM algorithm for Gaussian mixtures is a non-trivial algorithm to get working properly: please try and debug
it carefully. Check that the likelihood is non-decreasing at each step (i.e.,
have your code print it out as it goes through each iteration---if the
log-likelihood ever decreases you have a bug in your code). You could also
 write out the final parameters for the components and check that
the estimated parameters are roughly equal to the true parameters. 
%Note that
%you should also be able to partially reuse some of the code you have already
%written in earlier homeworks for Gaussian classifiers.



There are a number of options in implementing this algorithm
which I will leave up to you to determine. Specifically:
\begin{enumerate}
\item {\bf Convergence}: you will have to decide when its no longer worth iterating,
e.g., from a practical viewpoint it may not be worth continuing if the increase
in log-likelihood between the last 2 iterations is less than (say) 0.001\% of
the change in likelihood between the current value and the log-likelihood value
after the very first iteration of the algorithm. Another option here for detecting convergence is to compare the values in the $n \times K$ membership matrix from one iteration to the next, e.g., halt iterations if the average absolute change per probability is less than $10^{-5}$ for example.
For these data sets you could
also impose a maximum number of iterations to halt the algorithm (e.g.,  
500) if it gets that far and still has not converged. This is particularly
handy when debugging! 

\item {\bf Avoiding singular solutions}: the likelihood can go to
$\infty$ if the determinant of the covariance matrix for any component goes to zero (e.g., if any
individual $\sigma_{ii} \to 0$). You will need to implement some scheme to prevent
such singular (and useless) solutions. This is typically only a problem in
practice on small data sets. One somewhat ad hoc workaround (that works well in
practice)   is to constrain all covariance diagonal terms during the M-step to
be greater than some small threshold $\epsilon$ (e.g., $10^{-4}$ times the
variance for that dimension, as calculated on the whole data). A simple way to
do this is to calculate the $\Sigma$'s in the standard M-step manner  and then
to check each diagonal entry: if any are less than the threshold, then replace
them with the threshold. 
\end{enumerate}

\subsection*{General Advice on MATLAB implementations}
In coding your algorithms you should try to make your code as modular as
possible, by defining relatively short general functions that can be used in
different places in the code. For example, you can define a simple function
called {\tt euclid.m} that calculates the Euclidean distance between a  vector
$x$ of dimension $1 \times d$ and each row of a matrix of dimension $n
\times d$. This is a basic function in the $K$-means algorithm. Similarly
you can define specific functions for initialization, the E-step, the M-step,
etc., for Gaussian mixtures below, where each of these functions may call more
basic functions, such as a function that evaluates the Gaussian density values for
all rows of a data matrix given a set of Gaussian parameters. By making your code
modular in this fashion you can test individual functions independently from
the rest of the code, your code will be much easier to read and debug, and you
will be able to re-use the same functions in different places.

You should also write your equations in MATLAB using vector and matrix notation
where possible, and avoid for-loops when you can. This will make your code
simpler and faster.

For exchanging information about parameters between functions (e.g., sending the parameters learned by the EM learning algorithm to a different function to plot them) you may want to use a {\tt structure array} in MATLAB, with fields such as {\tt gparams(1).mean}, {\tt gparams(1).covariance}, etc.

The MATLAB directory for this homework contains a simple
template for MATLAB code to run Gaussian mixtures---you do not need to necessarily use this, it is just provided here to give you an idea of how you might set up your code. 



%\subsection*{Optional Extra Credit (worth 30\% extra)}
\subsection*{Algorithm 3: Automatically Selecting $K$ for Gaussian Mixture Clustering}
There are a number of different methods for trying to find $K$ automatically
from the data. Essentially we are trying to the find the $K$ that gives the
best predictions on new data. A very simple  approach (but useful) for doing
this is   the BIC criterion, i.e, choose $K$ such that
\[
l(D|\hat{\theta}) - {{p_K}\over 2} \log n
\]
is maximized, where $l(D|\hat{\theta})$ is the maximizing value of the
log-likelihood as found by EM using data $D$ with $K$ components (this value of
the log-likelihood can be returned by your Gaussian mixture code), $p_K$ is the
total number of parameters in mixture model $K$ (you will need to compute this
for each $K$), and  $n$ is the number of data points in $D$. There is a rather
general theoretical justification for this BIC criterion which loosely speaking
says that as $K$ increases we should penalize the log-likelihood of the model
with $K$ components according to its increased complexity, and ${{p_K}\over 2}
\log n$ can be shown to be a good approximation to a true Bayesian penalty
term. For mixture models the theory does not hold in general, but nonetheless
it can be a useful approximate technique for model selection with mixtures.

To write a MATLAB program to do this is very simple given
your EM Algorithm  that you have
already written. Simply have a loop that runs the EM
algorithm with values of $K$ going from 1 to some $K_{\max}$
where $K_{\max}$ is selected by the user. Note that
$K=1$ is important: it might be the case that the data
are best explained by a single Gaussian! (for the case
of $K=1$ you obviously don't need to run EM). Your
code should return a list, for each value of $K$,
of both the log-likelihood (which should increase
monotonically as $K$ increases), the BIC criterion
as defined above, and the $K$ value that maximizes
the BIC criterion. For some of the smaller
data sets (e.g., data set 3) you may find
that as $K$ increases you have more problems both
with local maxima of the likelihood function and with
singular solutions (so you may need to either
make $K_{\max}$ smaller, and/or run a larger number of
random restarts).

There are other options besides BIC for selecting $K$ for mixture models, if  you are interested in this problem of model selection
(purely for your own interest, will not be graded, but feel free to submit results of experiments with these methods if you wish---only do this is if you have time and if you are sure you have completed the rest of the assignment):
\begin{enumerate}
\item A data-driven approach us to use cross-validation
on the log-likelihood scores for different $K$ values (as described `Model selection for probabilistic clustering using cross-validated', P. Smyth, {\it Statistics and Computing}, 2000. 
\item Another option is to use {\it nonparametric Bayesian methods}, which essentially assume that the number of mixture components could be infinitely large and then infer likely values for  $K$ conditioned on the observed data. In the Murphy text he described a collapsed Gibbs sampling method for Gaussian mixtures using this approach, in Algorithm 25.1.
\end{enumerate}



%
%\newpage
%
%\section*{Extra Credit (worth 25\% extra)}
%
%{\it Nonparametric Bayesian methods} are a class of statistical modeling techniques that allow us to model data with mixture models without fixing the value of $K$ in advance. Pages 882 to 887 in the text describe how these ideas can be applied to Gaussian mixture modeling. If you choose to do the extra credit assignment you need to do the following:
%\begin{itemize}
%\item Write and debug in Matlab the Collapsed Gibbs Sampler for DP mixtures, as described in Algorithm 25.1 in the text.
%\item Run the algorithm on each of data sets 1, 2, and 3. Plot the results in 2 dimensions, and plot the posterior over $K$. See Figure 25.7 in the text for an example.
%\item Write a short 2 to 3 page report summarizing your results.
%\end{itemize}
%
%Please note that this assignment is non-trivial so I strongly recommend you only tackle this (if you decide to) after you have finished the main part of the assignment. Also, you will be somewhat on your own on this extra credit problem, e.g., in answering student questions related to this assignment I will give priority to questions related to the main part of the assignment and might not have time to look into questions for the extra credit part.


\newpage

\section*{What to Submit}

 
\begin{itemize} \item Submit your MATLAB  code to the
Homework 5 dropbox in EEE, including a script that loads each data set and runs your code on each data set. Please upload all of your functions in a single compressed
file (please use .gz or zip format). If you are using something other than MATLAB please also provide a README file to explain to us how to use your code.

\item Submit a written hardcopy summary of your results in class with the information below.
\end{itemize}


 Please try to put multiple plots on the same page using ``subplots" in MATLAB
(where different pages could correspond to different data sets and different
algorithms), to avoid printing lots of pages. For each data set, using the
true $K$ value for each one, show the following
\begin{enumerate}
\item The $K$-means solution (scatter plot in
 two dimensions (any two dimensions for Data Set 4)
 illustrating the location of the solution (i.e.,
 the cluster means), and plotting the data from different
 clusters with different symbols (and/or in color if you would like to use color).
\item A plot of the sum-squared-error (divided by $n$) as a function of
iteration number in the $K$-means algorithm. 
\item The initial parameter values
and the final parameter values (2 plots, each showing means and covariances for
each cluster) for the EM/Gaussian mixtures code  for the
highest-likelihood solution. You can use or modify the code from Homework 1 to do the plotting. 

\item A plot of the log-likelihood (for one run of your algorithm) as a
function of iteration number during EM. 

\item Add some brief comments (1
paragraph) on the difference between $K$-means and EM for each data set. 
\item  Generate a table of log-likelihood and BIC scores for
$K$ going from $K=1$ to some maximum value (e.g., $K=5$). Comment briefly on the results.  
%\item For data set 4, generate an image where you replace each original pixel value with the mean of the cluster it is assigned to by $K$-means, for $K=5, 10, 15$. This is essentially how image data compression works. Comment on the visual quality of the ``compressed" images (with the pixel rgb values replaced by their cluster means) compared to the original images.

%\item An additional 2 to 3 page report for the extra credit portion, if you do the extra credit part. Your extra credit report should be clearly labeled with "ExtraCredit" in the title.

\end{enumerate}




\end{document}
